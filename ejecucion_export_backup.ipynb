{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import concurrent.futures\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "project_id = 'iter-data-storage-pv-uat'\n",
    "dataset_id = 'acsele_data_bk'\n",
    "bucket_name='interseguro-datalake-alloy-uat-new'\n",
    "path_insert='Desarrollo/Replicacion_insert/'\n",
    "dataset_output='acsele_temp'\n",
    "dataset_input='acsele_alloy_new'\n",
    "path_ddl='Desarrollo/Replicacion_ddl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql =f\"\"\"\n",
    "SELECT \n",
    "TABLE_CATALOG\n",
    ",TABLE_SCHEMA\n",
    ",TABLE_NAME\n",
    ",REPLACE(REPLACE(REPLACE(DDL,'{dataset_id}','acsele_temp'),');',CONCAT(')OPTIONS (format=\"PARQUET\",URIS=[\"gs://{bucket_name}/{dataset_id}/',TABLE_NAME,'/*.parquet\"]);')),'CREATE TABLE','CREATE OR REPLACE EXTERNAL TABLE') AS DDL\n",
    "FROM {project_id}.{dataset_id}.INFORMATION_SCHEMA.TABLES\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blobs(dataset,table_name):\n",
    "    client_storage = storage.Client(project=project_id)\n",
    "    bucket = client_storage.bucket(bucket_name)\n",
    "    blobs = bucket.list_blobs(prefix=f'{dataset}/{table_name}')\n",
    "    return blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_blob(blob):\n",
    "    blob.delete()\n",
    "    print(f'Archivo {blob.name} eliminado.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_storage(dataset_id,project_id):\n",
    "    client = bigquery.Client(project_id)\n",
    "    tables = client.list_tables(dataset_id)\n",
    "    for table in tables:\n",
    "        table_id = table.table_id\n",
    "        blobs = generate_blobs(dataset_id,table_id)\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(delete_blob, blob) for blob in blobs]\n",
    "            concurrent.futures.wait(futures)  \n",
    "    return f\"Limpieza ruta: {dataset_id}/{table_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_table_storage(dataset_id,project_id):\n",
    "    client = bigquery.Client(project_id)\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    tables = client.list_tables(dataset_ref)\n",
    "    for table in tables:\n",
    "        table_id = table.table_id\n",
    "        table_ref = dataset_ref.table(table_id)\n",
    "        uri = f'gs://{bucket_name}/{dataset_id}/{table_id}/*.parquet'\n",
    "        extract_job = client.extract_table(\n",
    "            table_ref,\n",
    "            uri,\n",
    "            location=\"US\",\n",
    "            job_config=bigquery.ExtractJobConfig(destination_format=\"PARQUET\")\n",
    "            )\n",
    "        print(f'Tabla Exportada: {table_ref}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table_bq(query):\n",
    "  df = pd.read_gbq(query, project_id=project_id, dialect='standard')\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlbigquery(file):\n",
    "    with open(file,\"r\") as file:\n",
    "        statement = file.readlines()\n",
    "        statement = \" \".join(statement)\n",
    "    return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_job(query,project_id):\n",
    "    client = bigquery.Client(project_id)\n",
    "    query_job = client.query(query)\n",
    "    results = query_job.result()\n",
    "    for row in results:\n",
    "        table_name = row['TABLE_NAME']\n",
    "        ddl_statement = row['DDL']\n",
    "        client.query(ddl_statement).result()\n",
    "        print(f'Tabla creada: {table_name}')\n",
    "    return f'Ejectuado Correctamente'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_base(path,df):\n",
    "  for index, row in df.iterrows():\n",
    "    nombre_archivo = row['TABLE_NAME']\n",
    "    contenido_archivo = row['DDL']\n",
    "    with open(path + nombre_archivo+'.sql', 'w', newline='') as archivo:\n",
    "      archivo.write(contenido_archivo)\n",
    "  return 'Se exporto correctamente los DDL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_to_table(path,dataset_id,dataset_output,dataset_input):\n",
    "    client = bigquery.Client(project_id)\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    tables = client.list_tables(dataset_ref)\n",
    "    for table in tables:\n",
    "        table_id = table.table_id\n",
    "        table_ref = dataset_ref.table(table_id)\n",
    "        with open(path+table_id+'.sql','w') as file:\n",
    "            file.write('INSERT INTO '+project_id+'.'+dataset_input+'.'+table_id+'\\n')\n",
    "            file.write('SELECT * FROM '+project_id+'.'+dataset_output+'.'+table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_job_insert(path):\n",
    "    client = bigquery.Client(project_id)\n",
    "    list_sql = os.listdir(path)\n",
    "    for file in list_sql:\n",
    "        print(f\" Query insert a BQ: {file}\")\n",
    "        query = sqlbigquery(path+file)\n",
    "        query_job = client.query(query)\n",
    "        query_job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main que realiza el exportado a cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_export_storage():\n",
    "    clean_storage(dataset_id,project_id)\n",
    "    export_table_storage(dataset_id,project_id)\n",
    "    return \"Ejecutado Correctamente\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main que crea los ddl para las tablas externas en bq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_create_external_table():\n",
    "    store_base(path_ddl,read_table_bq(sql))\n",
    "    execute_job(sql,project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_export_storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_create_external_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genera los archivos sql con los insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_to_table(path_insert,dataset_id,dataset_output,dataset_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Query insert a BQ: CLAIM_raw.sql\n",
      " Query insert a BQ: COBERTURA_raw.sql\n"
     ]
    }
   ],
   "source": [
    "execute_job_insert(path_insert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta la exportacion a cloud storage del todo un dataset entero \n",
    "main_export_storage()\n",
    "# Crea los DDL para las tablas externas y los ejecuta en bq\n",
    "main_create_external_table()\n",
    "# genera los insert de las tablas externas a tabla en bq\n",
    "external_to_table(path_insert,dataset_id,dataset_output,dataset_input)\n",
    "# Aqui falta sumar el truncate table pero es riesgoso (evaluar)\n",
    "\n",
    "# Ejecuta todos los insert \n",
    "execute_job_insert(path_insert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
